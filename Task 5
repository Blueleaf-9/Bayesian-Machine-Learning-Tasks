In this paper, the authors introduce BatchBALD, a tractable approximation to the mutual information between a batch of points and model parameters, for the task of deep Bayesian active learning. Active learning (AL) is a technique that aims to minimize the amount of data that needs to be labeled by iteratively acquiring labels from an expert only for the most informative data points from a pool of available unlabeled data. BatchBALD is a greedy linear-time algorithm that selects multiple informative points jointly for the batch acquisition. The authors compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. They show that, using BatchBALD to consider dependencies within an acquisition batch, they achieve new state-of-the-art performance on standard benchmarks, providing substantial data efficiency improvements in batch acquisition. The authors discuss the relationship between active learning and Bayesian optimization and semi-supervised learning, and review related work in maintaining diversity when acquiring a batch of data points.
